{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Llama 3 and Gemma 2 locally with MLX\n",
    "  \n",
    "This notebook test drives the [mlx-lm library](https://qwen.readthedocs.io/en/latest/run_locally/mlx-lm.html) for optimizing local LLMs on Apple Silicon. We use small, quantized versions of the open source models Llama 3 and Gemma 2.  \n",
    "  \n",
    "These timings reflect running the notebook on an M1 MacBook Air (2020) with a wee 8GB of RAM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "from mlx_lm import generate, load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a generic function to interact with either LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_llm_mlx(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        prompt,\n",
    "        system_role=None,\n",
    "        prepend_system_role=False,\n",
    "        max_tokens=500,\n",
    "        verbose=True\n",
    "        ):\n",
    "    \"\"\"Ask an LLM a question and get a response!\"\"\"\n",
    "    if system_role:\n",
    "        if prepend_system_role:\n",
    "            # Some models like Gemma don't support system_role as a parameter\n",
    "            prompt = (\n",
    "                f\"SYSTEM ROLE: {system_role}{'.' if not system_role.endswith('.') else system_role} PROMPT: {prompt}\"\n",
    "                .replace(\"  \", \" \")\n",
    "                .strip()\n",
    "            )\n",
    "        else:\n",
    "            # Many local models like Llama support system_role explicitly.\n",
    "            prompt = tokenizer.apply_chat_template(\n",
    "                conversation=[\n",
    "                    {\"role\": \"system\", \"content\": system_role}, \n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                    ], \n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True\n",
    "        )\n",
    "    return Markdown(\n",
    "        generate(model, tokenizer, prompt, max_tokens=max_tokens, verbose=verbose)\n",
    "        .replace(\"ANSWER:\", \"\")\n",
    "        .replace(\"Answer:\", \"\")\n",
    "        .replace(\"<end_of_turn>\", \"\")\n",
    "        .strip(\". \") # Remove periods or spaces from either side\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Side note: the original prompt `What's the capital of Massachusetts?` led the model to generate multiple choice responses with different cities in Massacshueetts, like a quiz, and then select the correct answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü¶ô Llama 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load\n",
    "The first time you run the next line, it will download 5 GB of files for this version of Llama 3: 8B, instruction-tuned, 4-bit quantized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0915652bda3d464997645745c472ee36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_llama3_8b_it, tokenizer_llama3_8b_it = load(\"mlx-community/Meta-Llama-3-8B-Instruct-4bit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic invocation\n",
    "Let's see how Llama 3 on `mlx` behaves without specifying a system role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: What's the capital of Massachusetts?\n",
      " A) Boston B) Springfield C) Worcester D\n",
      "==========\n",
      "Prompt: 7 tokens, 0.367 tokens-per-sec\n",
      "Generation: 10 tokens, 0.047 tokens-per-sec\n",
      "Peak memory: 4.946 GB\n",
      "CPU times: user 198 ms, sys: 25.9 s, total: 26.1 s\n",
      "Wall time: 3min 30s\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "A) Boston B) Springfield C) Worcester D"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "invoke_llm_mlx(\n",
    "    model_llama3_8b_it,\n",
    "    tokenizer_llama3_8b_it,\n",
    "    \"What's the capital of Massachusetts?\",\n",
    "    max_tokens=10\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THe prompt led the model to respond with multiple-choice answers. It's interesting to see the slow inference time (0.047 tokens/sec) and 5 GB peak memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: Name the capital of Massachusetts\n",
      ".\n",
      "Answer: Boston....more\n",
      "What is\n",
      "==========\n",
      "Prompt: 5 tokens, 0.294 tokens-per-sec\n",
      "Generation: 10 tokens, 0.051 tokens-per-sec\n",
      "Peak memory: 6.317 GB\n",
      "CPU times: user 210 ms, sys: 25.5 s, total: 25.7 s\n",
      "Wall time: 3min 13s\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       " Boston....more\n",
       "What is"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "invoke_llm_mlx(\n",
    "    model_llama3_8b_it,\n",
    "    tokenizer_llama3_8b_it,\n",
    "    \"Name the capital of Massachusetts\",\n",
    "    max_tokens=10\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more optimal prompt yields a more direct answer, still wtih some fluff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a system role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a back-end GIS system. Answer queries with the answer only, no conversation or boilerplate<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Name the capital of Massachusetts<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Boston\n",
      "==========\n",
      "Prompt: 40 tokens, 1.074 tokens-per-sec\n",
      "Generation: 2 tokens, 0.053 tokens-per-sec\n",
      "Peak memory: 6.337 GB\n",
      "CPU times: user 99.9 ms, sys: 7.37 s, total: 7.47 s\n",
      "Wall time: 56.6 s\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Boston"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "invoke_llm_mlx(\n",
    "    model_llama3_8b_it,\n",
    "    tokenizer_llama3_8b_it,\n",
    "    prompt=\"Name the capital of Massachusetts\",\n",
    "    system_role=\"You are a back-end GIS system. Answer queries with the answer only, no conversation or boilerplate\",\n",
    "    max_tokens=10\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Substantially tighter with a precise role, with gross timing less due to fewer tokens to generate. The overall throughput is similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Respond in LaTeX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a physics database. Answer queries with the answer only, no conversation or boilerplate. If the answer requires a formula, use LaTeX<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Provide the formula for mass-energy equivalence<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "$E = mc^2$\n",
      "==========\n",
      "Prompt: 50 tokens, 1.284 tokens-per-sec\n",
      "Generation: 8 tokens, 0.061 tokens-per-sec\n",
      "Peak memory: 6.343 GB\n",
      "CPU times: user 168 ms, sys: 21.1 s, total: 21.2 s\n",
      "Wall time: 2min 33s\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "$E = mc^2$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "invoke_llm_mlx(\n",
    "    model_llama3_8b_it,\n",
    "    tokenizer_llama3_8b_it,\n",
    "    prompt=\"Provide the formula for mass-energy equivalence\",\n",
    "    system_role=\"You are a physics database. Answer queries with the answer only, no conversation or boilerplate. If the answer requires a formula, use LaTeX\",\n",
    "    max_tokens=50\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whimsy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a friendly robot.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Good morning<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Beep boop! Good morning to you too\n",
      "==========\n",
      "Prompt: 23 tokens, 0.777 tokens-per-sec\n",
      "Generation: 10 tokens, 0.051 tokens-per-sec\n",
      "Peak memory: 6.343 GB\n",
      "CPU times: user 233 ms, sys: 27.1 s, total: 27.3 s\n",
      "Wall time: 3min 27s\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Beep boop! Good morning to you too"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "invoke_llm_mlx(\n",
    "    model_llama3_8b_it,\n",
    "    tokenizer_llama3_8b_it,\n",
    "    prompt=\"Good morning\",\n",
    "    system_role=\"You are a friendly robot.\",\n",
    "    max_tokens=10\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíé Gemma 2\n",
    "Let's repeat the chats with a smaller model to compare responses and timings.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ca04039679e441f81116e93900c2dda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_gemma_2b_it, tokenizer_gemma_2b_it = load(\"mlx-community/gemma-2-2b-it-4bit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic invocation\n",
    "No system role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: Name the capital of Massachusetts\n",
      ".\n",
      "\n",
      "What is the name of the largest ocean\n",
      "==========\n",
      "Prompt: 6 tokens, 5.508 tokens-per-sec\n",
      "Generation: 10 tokens, 14.728 tokens-per-sec\n",
      "Peak memory: 7.654 GB\n",
      "CPU times: user 195 ms, sys: 427 ms, total: 622 ms\n",
      "Wall time: 1.74 s\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "What is the name of the largest ocean"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "invoke_llm_mlx(\n",
    "    model_gemma_2b_it,\n",
    "    tokenizer_gemma_2b_it,\n",
    "    \"Name the capital of Massachusetts\",\n",
    "    max_tokens=10\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting answer üòÅ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a system role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: SYSTEM ROLE: You are a back-end GIS system. Answer queries with the answer only, no conversation or boilerplate. PROMPT: Name the capital of Massachusetts\n",
      ". \n",
      "\n",
      "\n",
      "ANSWER: Boston \n",
      "<end_of_turn>\n",
      "==========\n",
      "Prompt: 34 tokens, 123.626 tokens-per-sec\n",
      "Generation: 10 tokens, 24.544 tokens-per-sec\n",
      "Peak memory: 7.654 GB\n",
      "CPU times: user 165 ms, sys: 174 ms, total: 339 ms\n",
      "Wall time: 643 ms\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "\n",
       " Boston \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "invoke_llm_mlx(\n",
    "    model_gemma_2b_it,\n",
    "    tokenizer_gemma_2b_it,\n",
    "    prompt=\"Name the capital of Massachusetts\",\n",
    "    system_role=\"You are a back-end GIS system. Answer queries with the answer only, no conversation or boilerplate\",\n",
    "    prepend_system_role=True,\n",
    "    max_tokens=10,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Respond in LaTeX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: SYSTEM ROLE: You are a physics database. Answer queries with the answer only, no conversation or boilerplate. If the answer requires a formula, use LaTeX. PROMPT: Provide the formula for mass-energy equivalence\n",
      ". \n",
      "\n",
      "ANSWER:  $E=mc^2$ \n",
      "<end_of_turn>\n",
      "==========\n",
      "Prompt: 44 tokens, 158.632 tokens-per-sec\n",
      "Generation: 17 tokens, 23.530 tokens-per-sec\n",
      "Peak memory: 7.654 GB\n",
      "CPU times: user 322 ms, sys: 302 ms, total: 624 ms\n",
      "Wall time: 959 ms\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "  $E=mc^2$ \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "invoke_llm_mlx(\n",
    "    model_gemma_2b_it,\n",
    "    tokenizer_gemma_2b_it,\n",
    "    prompt=\"Provide the formula for mass-energy equivalence\",\n",
    "    system_role=\"You are a physics database. Answer queries with the answer only, no conversation or boilerplate. If the answer requires a formula, use LaTeX\",\n",
    "    prepend_system_role=True,\n",
    "    max_tokens=50\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whimsy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: SYSTEM ROLE: You are a friendly robot.You are a friendly robot. PROMPT: Good morning\n",
      "! How are you feeling today?  \n",
      "<end_of_turn>\n",
      "==========\n",
      "Prompt: 21 tokens, 103.105 tokens-per-sec\n",
      "Generation: 10 tokens, 21.566 tokens-per-sec\n",
      "Peak memory: 7.654 GB\n",
      "CPU times: user 226 ms, sys: 208 ms, total: 434 ms\n",
      "Wall time: 623 ms\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "! How are you feeling today?  \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "invoke_llm_mlx(\n",
    "    model_gemma_2b_it,\n",
    "    tokenizer_gemma_2b_it,\n",
    "    prompt=\"Good morning\",\n",
    "    system_role=\"You are a friendly robot.\",\n",
    "    prepend_system_role=True,\n",
    "    max_tokens=10\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí° My lessons learned\n",
    "* `mlx-lm` is a neat library for inference on Apple Silicon.\n",
    "* These examples use default temperature etc which may affect results.\n",
    "* Gemma 2 2B was blazingly fast, plenty of throughput on a minimal local environment with only 8 GB of RAM\n",
    "    - It's said to run in only 1 GB of RAM, and the timings here are for a kernel that still had Llama3 8B clogging up RAM too\n",
    "    - It indeed it could be >100x faster than Llama3 8B (both 4bit quantized versions)\n",
    "* Gemma 2 doesn't support a system role. It wasn't trained on one. It has to be added to the prompt. This will especially matter when we start using LangChain.\n",
    "* As expected, the smaller model Gemma 2B gave some lower-quality results than Llama3 8B, but better prompts and system roles helped. More prompt engineering and tuning of system role?\n",
    "* Avoid running these two `mlx` open models without a system role, even for straightforward questions. They give stranger, slower answers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama-farm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
