{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics: Running Llama 3 and Gemma 2 locally\n",
    "\n",
    "This is a 'Hello World!' for local LLMs!  \n",
    "  \n",
    "It runs the open source models Llama 3 and Gemma 2 in a relatively minimal local environment. It uses small, quantized models through the MLX library, which is optimized for Apple Silicon.\n",
    "  \n",
    "These timings reflect running the notebook on an M1 MacBook Air (2020) with a wee 8GB of RAM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "from mlx_lm import generate, load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a generic function to interact with either LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_llm(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        prompt,\n",
    "        system_role=None,\n",
    "        prepend_system_role=False,\n",
    "        max_tokens=500,\n",
    "        verbose=True\n",
    "        ):\n",
    "    \"\"\"Ask an LLM a question and get a response!\"\"\"\n",
    "    if system_role:\n",
    "        if prepend_system_role:\n",
    "            # Some models like Gemma don't support system_role as a parameter\n",
    "            prompt = (\n",
    "                f\"SYSTEM ROLE: {system_role}{'.' if not system_role.endswith('.') else system_role} PROMPT: {prompt}\"\n",
    "                .replace(\"  \", \" \")\n",
    "                .strip()\n",
    "            )\n",
    "        else:\n",
    "            # Many local models like Llama support system_role explicitly.\n",
    "            prompt = tokenizer.apply_chat_template(\n",
    "                conversation=[\n",
    "                    {\"role\": \"system\", \"content\": system_role}, \n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                    ], \n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True\n",
    "        )\n",
    "    return Markdown(\n",
    "        generate(model, tokenizer, prompt, max_tokens=max_tokens, verbose=verbose)\n",
    "        .replace(\"ANSWER:\", \"\")\n",
    "        .replace(\"Answer:\", \"\")\n",
    "        .replace(\"<end_of_turn>\", \"\")\n",
    "        .strip(\". \") # Remove periods or spaces from either side\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü¶ô Llama 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load\n",
    "The first time you run the next line, it will download 5 GB of files for this version of Llama 3: 8B, instruction-tuned, 4-bit quantized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16cc75f4e0ae446e909db7c4fd4d6ee5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_llama3_8b_it, tokenizer_llama3_8b_it = load(\"mlx-community/Meta-Llama-3-8B-Instruct-4bit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic invocation\n",
    "No system role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: Name the capital of Massachusetts\n",
      ".\n",
      "Answer: Boston....more\n",
      "What is\n",
      "==========\n",
      "Prompt: 5 tokens, 0.235 tokens-per-sec\n",
      "Generation: 10 tokens, 0.046 tokens-per-sec\n",
      "Peak memory: 4.945 GB\n",
      "CPU times: user 221 ms, sys: 25.9 s, total: 26.1 s\n",
      "Wall time: 3min 36s\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       " Boston....more\n",
       "What is"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "invoke_llm(\n",
    "    model_llama3_8b_it,\n",
    "    tokenizer_llama3_8b_it,\n",
    "    \"Name the capital of Massachusetts\",\n",
    "    max_tokens=10\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Side note: the original prompt `What's the capital of Massachusetts?` led the model to generate multiple choice responses with different cities in Massacshueetts, like a quiz, and then select the correct answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a system role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a back-end GIS system. Answer queries with the answer only, no conversation or boilerplate<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Name the capital of Massachusetts<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Boston\n",
      "==========\n",
      "Prompt: 40 tokens, 1.132 tokens-per-sec\n",
      "Generation: 2 tokens, 0.054 tokens-per-sec\n",
      "Peak memory: 4.965 GB\n",
      "CPU times: user 79 ms, sys: 7.25 s, total: 7.33 s\n",
      "Wall time: 54.3 s\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Boston"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "invoke_llm(\n",
    "    model_llama3_8b_it,\n",
    "    tokenizer_llama3_8b_it,\n",
    "    prompt=\"Name the capital of Massachusetts\",\n",
    "    system_role=\"You are a back-end GIS system. Answer queries with the answer only, no conversation or boilerplate\",\n",
    "    max_tokens=10\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Substantially faster with a precise role"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Respond in LaTeX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a physics database. Answer queries with the answer only, no conversation or boilerplate. If the answer requires a formula, use LaTeX<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Provide the formula for mass-energy equivalence<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "$E = mc^2$\n",
      "==========\n",
      "Prompt: 50 tokens, 1.368 tokens-per-sec\n",
      "Generation: 8 tokens, 0.049 tokens-per-sec\n",
      "Peak memory: 4.971 GB\n",
      "CPU times: user 172 ms, sys: 22.1 s, total: 22.3 s\n",
      "Wall time: 2min 59s\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "$E = mc^2$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "invoke_llm(\n",
    "    model_llama3_8b_it,\n",
    "    tokenizer_llama3_8b_it,\n",
    "    prompt=\"Provide the formula for mass-energy equivalence\",\n",
    "    system_role=\"You are a physics database. Answer queries with the answer only, no conversation or boilerplate. If the answer requires a formula, use LaTeX\",\n",
    "    max_tokens=50\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whimsy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a friendly robot.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Good morning<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Beep boop! Good morning to you too\n",
      "==========\n",
      "Prompt: 23 tokens, 0.576 tokens-per-sec\n",
      "Generation: 10 tokens, 0.044 tokens-per-sec\n",
      "Peak memory: 4.971 GB\n",
      "CPU times: user 232 ms, sys: 30.2 s, total: 30.4 s\n",
      "Wall time: 4min 2s\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Beep boop! Good morning to you too"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "invoke_llm(\n",
    "    model_llama3_8b_it,\n",
    "    tokenizer_llama3_8b_it,\n",
    "    prompt=\"Good morning\",\n",
    "    system_role=\"You are a friendly robot.\",\n",
    "    max_tokens=10\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíé Gemma 2\n",
    "Let's repeat the chats with a smaller model to compare responses and timings.  \n",
    "  \n",
    "> ‚ÑπÔ∏è **Info:** The cell timings below are when the `Gemma 2` section was run in a fresh kernel, in which we did *not* first run the `Llama 3` section. When we load Llama 3 into RAM too, the Gemma timings were sometimes slower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01b4bba6e0014d4e888f72f57da1b4c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_gemma_2b_it, tokenizer_gemma_2b_it = load(\"mlx-community/gemma-2-2b-it-4bit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic invocation\n",
    "No system role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: Name the capital of Massachusetts\n",
      ".\n",
      "\n",
      "What is the name of the largest ocean\n",
      "==========\n",
      "Prompt: 6 tokens, 9.938 tokens-per-sec\n",
      "Generation: 10 tokens, 22.907 tokens-per-sec\n",
      "Peak memory: 1.501 GB\n",
      "CPU times: user 151 ms, sys: 213 ms, total: 364 ms\n",
      "Wall time: 1 s\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "What is the name of the largest ocean"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "invoke_llm(\n",
    "    model_gemma_2b_it,\n",
    "    tokenizer_gemma_2b_it,\n",
    "    \"Name the capital of Massachusetts\",\n",
    "    max_tokens=10\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting answer üòÅ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a system role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: SYSTEM ROLE: You are a back-end GIS system. Answer queries with the answer only, no conversation or boilerplate. PROMPT: Name the capital of Massachusetts\n",
      ". \n",
      "\n",
      "\n",
      "ANSWER: Boston \n",
      "<end_of_turn>\n",
      "==========\n",
      "Prompt: 34 tokens, 129.861 tokens-per-sec\n",
      "Generation: 10 tokens, 25.351 tokens-per-sec\n",
      "Peak memory: 1.534 GB\n",
      "CPU times: user 145 ms, sys: 69 ms, total: 214 ms\n",
      "Wall time: 619 ms\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "\n",
       " Boston \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "invoke_llm(\n",
    "    model_gemma_2b_it,\n",
    "    tokenizer_gemma_2b_it,\n",
    "    prompt=\"Name the capital of Massachusetts\",\n",
    "    system_role=\"You are a back-end GIS system. Answer queries with the answer only, no conversation or boilerplate\",\n",
    "    prepend_system_role=True,\n",
    "    max_tokens=10,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Respond in LaTeX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: SYSTEM ROLE: You are a physics database. Answer queries with the answer only, no conversation or boilerplate. If the answer requires a formula, use LaTeX. PROMPT: Provide the formula for mass-energy equivalence\n",
      ". \n",
      "\n",
      "ANSWER:  $E=mc^2$ \n",
      "<end_of_turn>\n",
      "==========\n",
      "Prompt: 44 tokens, 164.438 tokens-per-sec\n",
      "Generation: 17 tokens, 27.463 tokens-per-sec\n",
      "Peak memory: 1.545 GB\n",
      "CPU times: user 251 ms, sys: 99.8 ms, total: 351 ms\n",
      "Wall time: 851 ms\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "  $E=mc^2$ \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "invoke_llm(\n",
    "    model_gemma_2b_it,\n",
    "    tokenizer_gemma_2b_it,\n",
    "    prompt=\"Provide the formula for mass-energy equivalence\",\n",
    "    system_role=\"You are a physics database. Answer queries with the answer only, no conversation or boilerplate. If the answer requires a formula, use LaTeX\",\n",
    "    prepend_system_role=True,\n",
    "    max_tokens=50\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whimsy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: SYSTEM ROLE: You are a friendly robot.You are a friendly robot. PROMPT: Good morning\n",
      "! How are you feeling today?  \n",
      "<end_of_turn>\n",
      "==========\n",
      "Prompt: 21 tokens, 137.741 tokens-per-sec\n",
      "Generation: 10 tokens, 24.385 tokens-per-sec\n",
      "Peak memory: 1.545 GB\n",
      "CPU times: user 151 ms, sys: 64.2 ms, total: 215 ms\n",
      "Wall time: 522 ms\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "! How are you feeling today?  \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "invoke_llm(\n",
    "    model_gemma_2b_it,\n",
    "    tokenizer_gemma_2b_it,\n",
    "    prompt=\"Good morning\",\n",
    "    system_role=\"You are a friendly robot.\",\n",
    "    prepend_system_role=True,\n",
    "    max_tokens=10\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí° My lessons learned\n",
    "* MLX is a great library for inference on Apple Silicon.\n",
    "* Gemma 2 2B is said to run in only 1 GB of RAM, and indeed it could be >100x faster than Llama3 8B (both 4bit quantized versions)\n",
    "* Gemma 2 2B was plenty fast enough on a light local environment with only 8 GB of RAM, hitting several tokens per second. \n",
    "    - Except when the RAM was jammed, such as when Llama 3 was loaded into memory at the same time. \n",
    "* As expected, the smaller model Gemma 2B gave some lower-quality results than Llama3 8B. More prompt engineering and tuning of system role?\n",
    "* Avoid running these two open models without a system role, even for straightforward questions. Both gave responses that were stranger and slower."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama-farm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
